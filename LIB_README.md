## Compgraph
Библиотека реализует логику Map-Reduce для обработки потоковых данных.

### Интерфейс графа вычислений

Граф вычислений состоит из точек входа для данных и операций над ними.

Пример графа:
```python
graph = Graph.graph_from_iter('texts') \
    .map(operations.FilterPunctuation('text')) \
    .map(operations.LowerCase('text')) \
    .map(operations.Split('text')) \
    .sort(['text']) \
    .reduce(operations.Count('count'), ['text']) \
    .sort(['count', 'text'])
```

### Как запустить тесты?

Перед тем, как запустить тесты, нужно установить библиотеку.

```bash
# Устанавливаем библиотеку compgraph
(shad_env)$ pip install -e compgraph --force-reinstall

# Стал доступен модуль compgraph в интерпретаторе
# Теперь можете запустить тесты, которые используют модуль compgraph в импортах
(shad_env)$ pytest compgraph
```

### Задачи

#### Word Count
Требуется для каждого из слов, встречающихся в колонке `text`, посчитать количество вхождений во всю таблицу в сумме.

Файл с данными для этой и двух следующих задач: [`resource/text_corpus.txt`](resource/text_corpus.txt)

#### Инвертированный индекс на tf-idf

Для этой коллекции построим *инвертированный индекс* — структуру данных, которая для каждого слова хранит
список документов в котором оно встречается, отсортированный в порядке *релевантности*.

Релевантность будем считать по метрике [tf-idf](https://ru.wikipedia.org/wiki/TF-IDF).

#### Топ слов с наибольшей взаимной информацией

Задача, обратная предыдущей: для каждого документа посчитать топ-10 слов, наиболее характерных для него.

Ранжировать слова будем по метрике
[Pointwise mutual information](https://en.wikipedia.org/wiki/Pointwise_mutual_information).

Более формально задача ставится так: для каждого текста `doc_j` надо найти топ-10 слов `word_i`, удовлетворяющих двум условиям:
1) `word_i` строго длиннее четырех символов;
2) `word_i` встречается в документе `doc_j` не менее двух раз.
Топ слов `word_i` следует выбирать по величине:
```
pmi(word_i, doc_j) = ln((frequency of word_i in doc_j) / (frequency of word_i in all documents combined))
```
Чем эта величина выше, тем более характерным для `doc_j` считается `word_i`.

#### Средняя скорость движения по городу от часа и дня недели

В этой задаче вам надо работать с информацией о движении людей на машинах по какому-то подмножеству улиц города Москвы.

Улицы города заданы как граф, а информация о передвижении задана как таблица, в каждой строке которой данные вида
```
{'edge_id': '624', 'enter_time': '20170912T123410.1794', 'leave_time': '20170912T123412.68'}
```
где `edge_id` — идентификатор ребра графа дорог (то есть просто участка какой-то улицы), а `enter_time` и `leave_time` —
соответственно время въезда и выезда с/на это ребро (время в UTC).

Также вам дана вспомогательная таблица вида
```
{'edge_id': '313', 'length':121, 'start': [37.31245, 51.256734], 'end': [37.31245, 51.256734]}
```
где `length` - длина в метрах, `start` и `end` — координаты начала и конца ребра, заданные в формате `('lon', 'lat')`.
Быть может, не для всех рёбер графа есть вся метаинформация, поэтому расстояние вам стоит искать самостоятельно. 

Note: Расстояние между точками рассчитывается с помощью [haversine distance](https://en.wikipedia.org/wiki/Haversine_formula), радиус Земли стоит положить равным 6373 км (сверьтесь с тестами).

Файлы для этой задачи: [`resource/travel_times.txt`](resource/travel_times.txt)
и [`resource/road_graph_data.txt`](resource/road_graph_data.txt) 
